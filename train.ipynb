{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/data/FIBSEM/testing_scripts/sourceFile'\n",
    "mrc_file = 'TS_30_032123_tomo_bin4_flipped.mrc'\n",
    "annotation_masks = f'{source}/annotations/masks'\n",
    "annotation_imgs = f'{source}/annotations/images'\n",
    "cryo_flag = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b000259",
   "metadata": {},
   "source": [
    "#### Convert mrc to jpeg files. User mrc2tif or 3dmod. The images should be placed in source folder with name images.\n",
    "NOTE : predictions with NAD filtered images have shown better results. Use Etomo to create NAD mrc files from the source file and change the path to NAD filtered mrc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b241a7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = source + '/' +mrc_file\n",
    "!mkdir {source}/images\n",
    "!mrc2tif -j {path}  {source}/images/zap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a84d2e5",
   "metadata": {},
   "source": [
    "### Install required packages\n",
    "#### Qlty2d is used to create smaller slices of images.  https://qlty.readthedocs.io/en/latest/\n",
    "#### dlsia developed by LBNL is used to create custom MSDNETs and UNETs. https://dlsia.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269663d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!python3 -m pip install --upgrade Pillow\n",
    "!pip install tiffile\n",
    "!pip install qlty\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab5ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/phzwart/dlsia.git\n",
    "!cd dlsia && pip install -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa3aaf74",
   "metadata": {},
   "source": [
    "#### Pick 15-20 images for annotations. Use apeer or napari to generate annotations for the choosen images. \n",
    "The annotations should be placed in source folder with following path.\n",
    "-  {source}/annotations/images \n",
    "-  {source}/annotations/masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread, imwrite\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from dlsia.core import helpers, train_scripts, corcoef\n",
    "from dlsia.core.networks import msdnet, tunet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f60af0f",
   "metadata": {},
   "source": [
    "### For training the network the required format for images and masks are image.tiff and masks.tiff. a single tiff file for each with same sequence of images is required which is to be placed in annotations folder\n",
    "\n",
    "Apeer generates different files for each mask. Use the below code to generate images.tiff and masks.tiff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89be961d",
   "metadata": {},
   "source": [
    "### Use the below code to rename masks file with same naming convention to images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, filename in enumerate(os.listdir(annotation_masks)):\n",
    "    newfname = filename.replace(\"_filament.ome\", \"\")\n",
    "    os.rename(annotation_masks+'/'+filename,annotation_masks+'/'+newfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d17602",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imgs,training_masks = [], []\n",
    "\n",
    "for file in glob.glob(annotation_masks+\"/*.tiff\"):training_masks.append(file)\n",
    "for file in glob.glob(annotation_imgs+\"/*.jpg\"): training_imgs.append(file)\n",
    "    \n",
    "training_imgs = sorted(training_imgs)\n",
    "training_masks = sorted(training_masks)\n",
    "\n",
    "train_imgs = []\n",
    "for j in range(len(training_imgs)):\n",
    "    img = Image.open(training_imgs[j])\n",
    "    img.load()\n",
    "    img = np.array(img, dtype='float32')\n",
    "    if len(img.shape)==3: img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    train_imgs.append(img)\n",
    "\n",
    "train_masks = []\n",
    "for j in range(len(training_masks)):\n",
    "    img = Image.open(training_masks[j])\n",
    "    img.load()\n",
    "    img = np.array(img, dtype='uint8')\n",
    "    img[img!=0] = 1\n",
    "    train_masks.append(img)\n",
    " \n",
    "train_imgs, train_masks =np.array(train_imgs), np.array(train_masks)\n",
    "print(train_imgs.shape, train_imgs.dtype)\n",
    "print(train_masks.shape, train_masks.dtype)\n",
    "imwrite(f'{source}/annotations/train.tif', train_imgs)\n",
    "imwrite(f'{source}/annotations/masks.tif', train_masks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22770d99",
   "metadata": {},
   "source": [
    "### Use the below code for annotation masks where each class has one mask file.  if all the classes are present in single mask file use the above code. \n",
    "List down the classes that are annotated in the masks. This is used to combine multiple annotation files to a single image mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80074a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapper = {'ribosome':1,'tube':2,'mem':3,'filament':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7b301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapper= collections.defaultdict(list)\n",
    "\n",
    "for count, filename in enumerate(os.listdir(annotation_masks)):\n",
    "    mapper[filename.split('_')[-1]].append(filename)\n",
    "\n",
    "train_masks = []\n",
    "train_imgs = []\n",
    "\n",
    "for k,v in mapper.items():\n",
    "    x = np.array([])\n",
    "    \n",
    "    for name in v:\n",
    "        img = Image.open(f'{annotation_masks}/{name}')\n",
    "        img.load()\n",
    "        img= np.array(img, dtype='uint8')\n",
    "        \n",
    "        if not x.any(): x=img\n",
    "        for class_type, label in class_mapper.items():\n",
    "            if class_type in name:x[img!=0]=label \n",
    "    \n",
    "    train_masks.append(x)\n",
    "    \n",
    "    img = Image.open(f'{annotation_imgs}/{k}.jpg')\n",
    "    img.load()\n",
    "    img = np.array(img, dtype='float32')\n",
    "    if len(img.shape)==3: img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    train_imgs.append(img)\n",
    "    \n",
    "train_imgs, train_masks =np.array(train_imgs), np.array(train_masks)\n",
    "imwrite(f'{source}/annotations/train.tif', train_imgs)\n",
    "imwrite(f'{source}/annotations/masks.tif', train_masks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fb0e50a",
   "metadata": {},
   "source": [
    "### Use napari or any visualization to verify the generated tiff for image and masks alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5829cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.expand_dims(train_imgs, axis=1)\n",
    "train_masks = np.expand_dims(train_masks, axis=1)\n",
    "\n",
    "def shuffle_training(imgs, masks, seed=123):\n",
    "    x = np.arange(imgs.shape[0])\n",
    "    random.seed(seed)\n",
    "    random.shuffle(x)\n",
    "    return imgs[x,:], masks[x,:]\n",
    "\n",
    "train_imgs, train_masks = shuffle_training(train_imgs, train_masks)\n",
    "print(train_imgs.shape, train_imgs.dtype)\n",
    "print(train_masks.shape, train_masks.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24fc9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cryo_flag == True:\n",
    "    import qlty\n",
    "    from qlty import qlty2D\n",
    "\n",
    "    quilt = qlty2D.NCYXQuilt(X=train_imgs.shape[3],\n",
    "                            Y=train_imgs.shape[2],\n",
    "                            window=(256,256),\n",
    "                            step=(64,64),\n",
    "                            border=(10,10),\n",
    "                            border_weight=0)\n",
    "\n",
    "    labeled_imgs = torch.Tensor(train_imgs)\n",
    "    labeled_masks = torch.Tensor(train_masks)\n",
    "    labeled_imgs, labeled_masks = quilt.unstitch_data_pair(labeled_imgs,labeled_masks)\n",
    "\n",
    "    print(\"x shape: \",train_imgs.shape)\n",
    "    print(\"y shape: \",train_masks.shape)\n",
    "    print(\"x_bits shape:\", labeled_imgs.shape)\n",
    "    print(\"y_bits shape:\", labeled_masks.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "401a7611",
   "metadata": {},
   "source": [
    "#### the below code adds additional annotations from different datasets. \n",
    "input is a list of folders with same convention of annotations(images and masks folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_dirs=['/data/FIBSEM/cryo/filament_Cryo/reannotate_masks','/data/FIBSEM/testing_scripts/sourceFile']\n",
    "\n",
    "for dirs in  additional_dirs: \n",
    "    add_imgs,add_masks= [],[]\n",
    "    imgs,masks = imread(dirs + '/train.tif'),imread(dirs + '/masks.tif')\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        add_imgs.append(img.astype(np.float32))\n",
    "        add_masks.append(masks[i])\n",
    "\n",
    "    add_imgs,add_masks=np.array(add_imgs),np.array(add_masks)\n",
    "    add_imgs,add_masks = np.expand_dims(add_imgs, axis=1),np.expand_dims(add_masks, axis=1)\n",
    "    add_imgs,add_masks = shuffle_training(add_imgs, add_masks)\n",
    "\n",
    "    labeled_actin_imgs,labeled_actin_masks  = torch.Tensor(add_imgs),torch.Tensor(add_masks)\n",
    "    if cryo_flag == True:\n",
    "        quilt = qlty2D.NCYXQuilt(X=add_imgs.shape[3],\n",
    "                                Y=add_masks.shape[2],\n",
    "                                window=(256,256),\n",
    "                                step=(64,64),\n",
    "                                border=(10,10),\n",
    "                                border_weight=0)\n",
    "\n",
    "        labeled_actin_imgs, labeled_actin_masks = quilt.unstitch_data_pair(labeled_actin_imgs,labeled_actin_masks)\n",
    "    print(\"x shape: \",add_imgs.shape)\n",
    "    print(\"y shape: \",add_masks.shape)\n",
    "    print(\"x_bits shape:\", labeled_actin_imgs.shape)\n",
    "    print(\"y_bits shape:\", labeled_actin_masks.shape)\n",
    "\n",
    "    labeled_imgs, labeled_masks = torch.Tensor(np.vstack((labeled_imgs,labeled_actin_imgs))), torch.Tensor(np.vstack((labeled_masks,labeled_actin_masks)))\n",
    "print(\"images shape:\", labeled_imgs.shape)\n",
    "print(\"masks shape:\", labeled_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6475de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cryo_flag == True:\n",
    "    dicedImgs,dicedMasks = [],[]\n",
    "    for i in range(len(labeled_imgs)):\n",
    "        # comment this to include all slices even the non annotated slices. \n",
    "        if np.unique(labeled_masks[i][0]).shape[0] > 1:\n",
    "            # bilateral filter\n",
    "            bilateral = cv2.bilateralFilter(labeled_imgs[i][0].numpy(),5,50,10)\n",
    "            # clahe equalization \n",
    "            clahe = cv2.createCLAHE(clipLimit=3)\n",
    "            bilateral= bilateral.astype(np.uint16)\n",
    "            final = clahe.apply(bilateral)\n",
    "            # Equalize histogram \n",
    "            x = exposure.equalize_hist(final)\n",
    "            dicedImgs.append(x.astype(np.float32))\n",
    "            dicedMasks.append(labeled_masks[i][0].numpy())\n",
    "            \n",
    "    # verify random slice        \n",
    "    sliceNum = 250\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(dicedMasks[sliceNum],cmap='gray',interpolation='none')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(dicedImgs[sliceNum],cmap='gray',interpolation='none')\n",
    "    plt.show()\n",
    "    train_imgs,train_masks = np.array(dicedImgs),np.array(dicedMasks)\n",
    "    train_imgs,train_masks = np.expand_dims(train_imgs, axis=1),np.expand_dims(train_masks, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d567194",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_imgs, test_imgs, labeled_masks, test_masks = train_test_split(train_imgs, train_masks, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e311630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(train_data, val_data, test_data, \n",
    "                batch_size_train, batch_size_val, batch_size_test):\n",
    "    \n",
    "    # can adjust the batch size depending on available memory\n",
    "    train_loader_params = {'batch_size': batch_size_train,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "\n",
    "    val_loader_params = {'batch_size': batch_size_val,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "\n",
    "    test_loader_params = {'batch_size': batch_size_test,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "\n",
    "    train_loader = DataLoader(train_data, **train_loader_params)\n",
    "    val_loader = DataLoader(val_data, **val_loader_params)\n",
    "    test_loader = DataLoader(test_data, **test_loader_params)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76ab9504",
   "metadata": {},
   "source": [
    "### Use the below only to augment data. Helpful when training data annotations are not enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_imgs = torch.Tensor(labeled_imgs)\n",
    "labeled_masks = torch.Tensor(labeled_masks)\n",
    "\n",
    "rotated_imgs1 = torch.rot90(labeled_imgs, 1, [2, 3])\n",
    "rotated_masks1 = torch.rot90(labeled_masks, 1, [2, 3])\n",
    "\n",
    "rotated_imgs2 = torch.rot90(labeled_imgs, 2, [2, 3])\n",
    "rotated_masks2 = torch.rot90(labeled_masks, 2, [2, 3])\n",
    "\n",
    "rotated_imgs3 = torch.rot90(labeled_imgs, 3, [2, 3])\n",
    "rotated_masks3 = torch.rot90(labeled_masks, 3, [2, 3])\n",
    "\n",
    "flipped_imgs1 = torch.flip(labeled_imgs, [2])\n",
    "flipped_masks1 = torch.flip(labeled_masks, [2])\n",
    "\n",
    "flipped_imgs2 = torch.flip(labeled_imgs, [3])\n",
    "flipped_masks2 = torch.flip(labeled_masks, [3])\n",
    "\n",
    "flipped_imgs3 = torch.flip(labeled_imgs, [2,3])\n",
    "flipped_masks3 = torch.flip(labeled_masks, [2,3])\n",
    "\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs1),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks1),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs2),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks2),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, rotated_imgs3),0)\n",
    "labeled_masks = torch.cat((labeled_masks, rotated_masks3),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs1),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks1),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs2),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks2),0)\n",
    "\n",
    "labeled_imgs = torch.cat((labeled_imgs, flipped_imgs3),0)\n",
    "labeled_masks = torch.cat((labeled_masks, flipped_masks3),0)\n",
    "\n",
    "print('Shape of augmented data:    ', labeled_imgs.shape, labeled_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create validation set \n",
    "\n",
    "num_val = int(0.1*labeled_imgs.shape[0])\n",
    "print('Number of images for validation: '+ str(num_val))\n",
    "val_imgs = labeled_imgs[-num_val:,:,:]\n",
    "val_masks = labeled_masks[-num_val:,:,:]\n",
    "train_imgs = labeled_imgs[:-num_val,:,:]   # actual training\n",
    "train_masks = labeled_masks[:-num_val,:,:]   # actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of training data:   ', train_imgs.shape)\n",
    "print('Size of validation data: ', val_imgs.shape)\n",
    "print('Size of testing data:    ', test_imgs.shape)\n",
    "\n",
    "num_labels = np.unique(train_masks[200:400,:])\n",
    "print('The unique mask labels: ', num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data in pytorch Dataset format\n",
    "train_data = TensorDataset(torch.Tensor(train_imgs), torch.Tensor(train_masks))\n",
    "val_data = TensorDataset(torch.Tensor(val_imgs), torch.Tensor(val_masks))\n",
    "test_data = TensorDataset(torch.Tensor(test_imgs), torch.Tensor(test_masks))\n",
    "\n",
    "# create data loaders\n",
    "num_workers = 0   # 1 or 2 work better with CPU, 0 best for GPU\n",
    "\n",
    "# change batch size based on memory available \n",
    "batch_size_train = 1\n",
    "batch_size_val = 1\n",
    "batch_size_test = 1\n",
    "\n",
    "train_loader, val_loader, test_loader = make_loaders(train_data,\n",
    "                                                    val_data,\n",
    "                                                    test_data,\n",
    "                                                    batch_size_train, \n",
    "                                                    batch_size_val, \n",
    "                                                    batch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSDNET \n",
    "\n",
    "in_channels = 1\n",
    "out_channels = len(num_labels)\n",
    "num_layers = 40          \n",
    "layer_width = 1\n",
    "max_dilation = 15      \n",
    "activation = nn.ReLU()\n",
    "normalization = nn.BatchNorm2d\n",
    "final_layer = None\n",
    "\n",
    "msd_net = msdnet.MixedScaleDenseNetwork(in_channels = in_channels,\n",
    "                                    out_channels = out_channels, \n",
    "                                    num_layers=num_layers, \n",
    "                                    layer_width=layer_width,\n",
    "                                    max_dilation = max_dilation, \n",
    "                                    activation=activation,\n",
    "                                    normalization=normalization,\n",
    "                                    convolution=nn.Conv2d\n",
    "                                   )\n",
    "\n",
    "print('Number of parameters: ', helpers.count_parameters(msd_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22479ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNET \n",
    "\n",
    "\n",
    "depth = 4\n",
    "base_channels = 64\n",
    "growth_rate = 2\n",
    "hidden_rate = 1\n",
    "in_channels = 1\n",
    "out_channels = len(num_labels)\n",
    "num_layers = 40             \n",
    "layer_width = 1 \n",
    "max_dilation = 15 \n",
    "normalization = nn.BatchNorm2d\n",
    "\n",
    "tunet3 = tunet.TUNet(image_shape=(train_imgs.shape[2:4]),\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            depth=depth,\n",
    "            kernel_down=nn.AvgPool2d,\n",
    "            base_channels=base_channels,\n",
    "            normalization = nn.BatchNorm2d,\n",
    "            growth_rate=growth_rate,\n",
    "            hidden_rate=hidden_rate\n",
    "            )\n",
    "\n",
    "print('Number of parameters: ', helpers.count_parameters(tunet3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = helpers.get_device()\n",
    "device = \"cuda:1\"\n",
    "epochs = 60   # Set number of epochs\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()   # For segmenting >2 classes\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "optimizer_msd = optim.Adam(msd_net.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_tunet3 = optim.Adam(tunet3.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print('Device we will compute on: ', device)   # cuda:0 for GPU. Else, CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1686f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "newds_path = source+'/Results'\n",
    "if os.path.isdir(newds_path) is False: os.mkdir(newds_path)\n",
    "    \n",
    "model_msdnet = '/msdnet'\n",
    "model_tunet3 = '/tunet3'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b28c06ac",
   "metadata": {},
   "source": [
    "### Train MSDNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_net.to(device)   # send network to GPU\n",
    "\n",
    "main_dir = newds_path + model_msdnet\n",
    "if os.path.isdir(main_dir) is False: os.mkdir(main_dir)\n",
    "     \n",
    "stepsPerEpoch = np.ceil(train_imgs.shape[0]/batch_size_train)\n",
    "num_steps_down = 2\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_msd,\n",
    "                                 step_size=int(stepsPerEpoch*(epochs/num_steps_down)),\n",
    "                                 gamma = 0.1,verbose=False)\n",
    "\n",
    "msd_net, results = train_scripts.train_segmentation(\n",
    "            msd_net,train_loader, val_loader, epochs, \n",
    "            criterion, optimizer_msd, device,saveevery=3,scheduler=scheduler,savepath=main_dir,show=1)   # training happens here\n",
    "\n",
    "# clear out unnecessary variables from device (GPU) memory\n",
    "torch.cuda.empty_cache()\n",
    "torch.save(msd_net.state_dict(), main_dir + '/net')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(results['Training loss'], linewidth=2, label='training')\n",
    "plt.plot(results['Validation loss'], linewidth=2, label='validation')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MSDNet with ReLU and BatchNorm')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(main_dir + '/losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2338b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 10\n",
    "batch_size_val = 10\n",
    "batch_size_test = 10\n",
    "\n",
    "tunet3.to(device)   # send network to GPU\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "main_dir = newds_path + model_tunet3\n",
    "if os.path.isdir(main_dir) is False: os.mkdir(main_dir)\n",
    "\n",
    "stepsPerEpoch = np.ceil(train_imgs.shape[0]/batch_size_train)\n",
    "num_steps_down = 2\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_tunet3,\n",
    "                                 step_size=int(stepsPerEpoch*(epochs/num_steps_down)),\n",
    "                                 gamma = 0.1,verbose=False)\n",
    "\n",
    "tunet3, results = train_scripts.train_segmentation(\n",
    "    tunet3,train_loader, val_loader, epochs, \n",
    "    criterion, optimizer_tunet3, device,saveevery=3,\n",
    "    #scheduler=scheduler,\n",
    "    show=1)   # training happens here\n",
    "\n",
    "# clear out unnecessary variables from device (GPU) memory\n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "torch.save(tunet3.state_dict(), main_dir + '/net')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(results['Training loss'], linewidth=2, label='training')\n",
    "plt.plot(results['Validation loss'], linewidth=2, label='validation')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('TUnet with ReLU and BatchNorm')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(main_dir + '/losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3312c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tunet3.state_dict(), main_dir + '/net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'image_shape': train_imgs.shape[2:4], 'in_channels': in_channels, 'out_channels': out_channels, 'depth': depth, 'base_channels': base_channels, 'growth_rate': growth_rate, 'hidden_rate': hidden_rate},\n",
    "np.save(main_dir+'/params.npy',params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fe8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics( preds, target):\n",
    "    tmp = corcoef.cc(preds.cpu().flatten(), target.cpu().flatten() )\n",
    "    return(tmp)\n",
    "\n",
    "def segment_imgs(testloader, net):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    seg_imgs = []\n",
    "    noisy_imgs = [] \n",
    "    counter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            noisy = batch\n",
    "            noisy = noisy[0]\n",
    "            noisy = torch.FloatTensor(noisy)\n",
    "            noisy = noisy.to(device)#.unsqueeze(1)\n",
    "            output = net(noisy)\n",
    "            if counter == 0:\n",
    "                seg_imgs = output.detach().cpu()\n",
    "                noisy_imgs = noisy.detach().cpu()\n",
    "            else:\n",
    "                seg_imgs = torch.cat((seg_imgs, output.detach().cpu()), 0)\n",
    "                noisy_imgs = torch.cat((noisy_imgs, noisy.detach().cpu()), 0)\n",
    "            counter+=1\n",
    "    torch.cuda.empty_cache()\n",
    "    return seg_imgs, noisy_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "msdnet_output, noisy  = segment_imgs(test_loader, msd_net)\n",
    "msdnet_output = torch.argmax(msdnet_output.cpu()[:,:,:,:].data, dim=1)\n",
    "print(msdnet_output.size())\n",
    "noisy = torch.squeeze(noisy,1)\n",
    "imwrite(newds_path + '/tunet3_output.tif', msdnet_output.numpy())\n",
    "imwrite(newds_path + '/input.tif', noisy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4044db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunet3_output, noisy  = segment_imgs(test_loader, tunet3)\n",
    "tunet3_output = torch.argmax(tunet3_output.cpu()[:,:,:,:].data, dim=1)\n",
    "print(tunet3_output.size())\n",
    "noisy = torch.squeeze(noisy,1)\n",
    "imwrite(newds_path + '/tunet3_output.tif', tunet3_output.numpy())\n",
    "imwrite(newds_path + '/input.tif', noisy.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
